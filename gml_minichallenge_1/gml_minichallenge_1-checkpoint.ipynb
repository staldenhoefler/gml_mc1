{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09f5d29bc85d33abcebca0f92b120733",
     "grade": false,
     "grade_id": "cell-5f65801aa881010a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# GML - Mini-Challenge 1 - HS 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "828223eb6c99f921b3856fe356d5f2e4",
     "grade": false,
     "grade_id": "cell-2e8ceefd22c89a3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Ausgabe:** Montag, 16. Oktober 2023  \n",
    "**Abgabe:** Sonntag, 13. November 2023, bis 24 Uhr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f82f9c065f131a2e04e575d8bd5b0f3b",
     "grade": false,
     "grade_id": "cell-25d029424f3ee6b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In dieser Mini-Challenge implementieren und verwenden wir verschiedene Supervised Learning-Methoden zur Regression und machen Gebrauch von Model Selection-Prinzipien und -Algorithmen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64fe1fa6e6314b19c638caa03955ecbd",
     "grade": false,
     "grade_id": "cell-3c170b634cbe4c0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "###### Vorgaben zu Umsetzung und Abgabe\n",
    "\n",
    "\n",
    "- Code muss in python geschrieben werden.\n",
    "- Wir entwickeln zahlreiche Algorithmen selber. Wenn nicht explizit anders verlangt, dürfen bloss die folgenden Bibliotheken verwendet werden: numpy, matplotlib, seaborn, pandas\n",
    "- Der Code muss von Anfang bis Ende lauffähig sein bei Ausführung im Docker-Container des Trainingcenters. Nur was korrekt ausführt wird bewertet.\n",
    "- Es darf kein Code ausgelagert werden.\n",
    "- Es dürfen keine zusätzlichen Files angelegt werden, die für das korrekte Ausführen des Codes notwendig sind. Auch für Daten nicht.\n",
    "\n",
    "\n",
    "- Sämtliche **Plots** sind **komplett beschriftet** (Titel, Achsen, Labels, Colorbar, ..), sodass der Plot einfach verstanden werden kann.\n",
    "- Zu jedem Plot gehört eine **kurze Diskussion, welche den Plot erklärt und die wichtigsten Einsichten die damit sichtbar werden festhält**.  \n",
    "\n",
    "\n",
    "- **Nur vollständig beschriftete Plots mit dazugehörender Diskussion werden bewertet.**\n",
    "\n",
    "\n",
    "- Als **Abgabe** zählt der letzte Commit in deinem Fork des Repos vor Abgabetermin.\n",
    "- Falls du die Mini-Challenge abgeben und bewerten lassen möchtest, so **schreibe eine kurze Email** an den Fachexperten (moritz.kirschmann@fhnw.ch) innert 2er Tage nach Abgabe.  \n",
    "\n",
    "\n",
    "- **Bitte lösche, dupliziere oder verschiebe die vorhandenen Zellen nicht**. Dies führt zu Problemen bei der Korrektur. Du darfst aber beliebig viele weitere Zellen hinzufügen.\n",
    "\n",
    "\n",
    "- Importiere Daten mit **relativen Pfaden** innerhalb des Repos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c2b082499b400579a71ce49759de2dd",
     "grade": false,
     "grade_id": "cell-2ad57e93c5376359",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Für die Erarbeitung der Lösung darf unter Studierenden zusammengearbeitet werden. Die Zusammenarbeit ist dabei aber auf konzeptionelle und algorithmische Fragen und Verständnisaspekte beschränkt.  \n",
    "\n",
    "**Es darf kein Code oder Text von anderen oder vom Internet kopiert werden. Wir kopierter Code oder Text identifiziert, so werden sämtliche betroffenen Aufgaben von allen Beteiligten mit 0 Punkten bewertet.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05a2b673a03d92917e9953606749fcae",
     "grade": false,
     "grade_id": "cell-ef5b83552f44af7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cd4e1d8dedd61303e16e364420f213b",
     "grade": false,
     "grade_id": "cell-d55ef83e8de91c4a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 1 (4 Punkte)\n",
    "\n",
    "**Ridge Regression** (siehe beispielsweise James et al., *An Introduction to Statistical Learning*, 2015, pp 215) ist eine regularisierte Form ($l_2$-Regularisierung) der Ordinary Least Squares (OLS) Kostenfunktion für die lineare Regression.  \n",
    "\n",
    "Die klassische Ridge Regression-Kostenfunktion für einen Datensatz $(x^{(i)}, y^{(i)})$ mit $x^{(i)} = (x_1^{(i)}, \\dots , x_p^{(i)})$ von $N$ Datenpunkten ist gegeben durch: \n",
    "\n",
    "\\begin{equation}\n",
    "J(\\beta) = \\sum_{i=1}^{N} (y^{(i)}-\\beta_0 - \\sum_{j=1}^{p} x^{(i)}_j\\beta_j)^2 + \\alpha\\sum_{j=1}^{p} \\beta_j^2 \n",
    "\\end{equation}\n",
    "\n",
    "$(\\beta_0, \\beta_1, \\dots, \\beta_p)$ sind dabei die Modellkoeffizienten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da1669d0f2fc8bea2fe99408f19706ee",
     "grade": false,
     "grade_id": "cell-369fe9287526d43a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Es gilt zu beachten, dass bei Ridge Regression der Achsenabschnitt, i.e. der Modellkoeffizient $\\beta_0$ nicht in den Penalty Term der Kostenfunktion eingeht. Das zeigt sich in obiger Gleichung durch die Summe von $i=1$ (nicht $i=0$) bis $p$ (Anzahl Prädiktoren)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "071832ab6f178a77b38a04def7ac335e",
     "grade": false,
     "grade_id": "cell-96d6448ac9c72b44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Für die Optimierung der Koeffizienten bei gegebenem Datensatz ergeben sich dadurch Implikationen für Gradient Descent und Normalengleichung.  \n",
    "\n",
    "Wenn man die **Input-Variablen standardisiert**, was **bei Regularisierung fast immer angezeigt** ist um sämtliche Variablen auf eine vergleichbare Skala (dimensionslose Standardabweichungen) zu bringen, und damit die zugehörigen Koeffizienten in ähnlichem Umfang zu regularisieren, ist es eine Möglichkeit das Optimierungsproblem für $\\beta_0$ und die restlichen Variablen zu separieren. $\\beta_0$ kann dann nämlich mit $\\beta_0 = \\frac{1}{N} \\sum_{i=1}^{N}y^{(i)}$ berechnet werden und ist so unregularisiert. Die Koeffizienten $(\\beta_1, \\dots, \\beta_p)$ werden dann mit Gradient Descent oder Normalengleichen optimiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff63947b84c27d880152dc0d879b8826",
     "grade": false,
     "grade_id": "cell-8504499756e5a8d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Möchte man alternativ alle Modell-Koeffizienten, inklusive $\\beta_0$, mittels Gradient Descent optimieren, so gilt es eine Fallunterscheidung bei der Berechnung des Gradienten zu machen für die Gradienten-Komponente $0$, welche zum Koeffizienten $\\beta_0$ gehört, und den verbleibenden Gradienten-Komponenten $1$ bis $p$, welche zu den Modellkoeffizienten $(\\beta_1, \\dots, \\beta_p)$ gehören. Dies deswegen, weil $\\beta_0$ nicht in die Strafterm-Summe der Kostenfunktion eingeht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2991ea0f2fe243349bf52037a3e5942",
     "grade": false,
     "grade_id": "cell-3694451c94993f92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Zu Gradient Descent für Ridge Regression\n",
    "\n",
    "Zur Verwendung von Gradient Descent muss der Gradient der Kostenfunktion berechnet werden. Der Gradient $\\nabla f(\\chi)$ einer Funktion $f(\\cdot)$ mehrerer ($m$) Variablen $\\chi = (\\chi_1, \\chi_2, \\dots, \\chi_m)$ ist gegeben durch:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f(\\chi) = \\Big( \\frac{\\partial f(\\chi)}{\\partial \\chi_1}, \\frac{\\partial f(\\chi)}{\\partial \\chi_2}, \\dots, \\frac{\\partial f(\\chi)}{\\partial \\chi_m}\\Big)\n",
    "\\end{equation}\n",
    "\n",
    "$\\frac{\\partial f(\\chi)}{\\partial \\chi_i}$ ist dabei die partielle Ableitung von $f(\\cdot)$ nach $\\chi_i$. $\\nabla f(\\chi)$ ist also ein $m$-dimensionaler Vektor.   \n",
    "\n",
    "Bei Standardisierung der Input-Variablen und separater 'Optimierung' von $\\beta_0$ wird $\\beta_0$ vorab berechnet, wird dann zur Konstanten in der Kostenfunktion, und muss nicht mehr mitoptimiert werden, i.e. kann beim Gradienten aussen vor gelassen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "077815c8fabb16fdac5cd7b5929f680c",
     "grade": false,
     "grade_id": "cell-757ad808dfaf3b56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Zur Normalengleichung für Ridge Regression\n",
    "\n",
    "Einen Input-Datensatz können wir als $N \\times p+1$ Matrix $\\mathbf{X}$ schreiben. $p+1$ deswegen, weil wir den $p$ Input-Variablen noch eine Spalte von $1$-en voranstellen können, um den Koeffizienten $\\beta_0$ mit berücksichtigen zu können. Sie hat also die Form\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "1 & x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_j & \\cdots & x^{(1)}_p \\\\\n",
    "1 & x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_j & \\cdots & x^{(2)}_p \\\\\n",
    "& & \\vdots & &\\\\\n",
    "1 & x^{(i)}_1 & x^{(i)}_2 & \\cdots & x^{(i)}_j & \\cdots & x^{(i)}_p \\\\\n",
    "& & \\vdots & &\\\\\n",
    "1 & x^{(n)}_1 & x^{(n)}_2 & \\cdots & x^{(n)}_j & \\cdots & x^{(n)}_p\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Gleichermassen können wir unsere Output-Variablen als $N$-dimensionalen Vektor $y = (y^{(1)}, .. y^{(N)})$ betrachten.  \n",
    "\n",
    "Damit können wir ein lineares Modell in kompakter Schreibweise wie folgt formulieren:  \n",
    "\n",
    "\\begin{equation}\n",
    "y = \\mathbf{X}\\beta + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "Wobei $\\epsilon = (\\epsilon_1, \\cdots \\epsilon_N)$ ein Vektor von irreduzierbaren Fehlern für die $N$ Datenpunkte ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a8ab4d9124745582a4f13b7c47cb1907",
     "grade": false,
     "grade_id": "cell-b94b99c5883923d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Für die **unregularisierte OLS Kostenfunktion kann eine analytische Lösung** gefunden werden, die als Normalengleichung bezeichnet wird:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^Ty\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Gleichermassen kann für die analytische Lösung der **Ridge Regression Kostenfunktion** folgende analytische Lösung hergeleitet werden:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (\\mathbf{X}^T \\mathbf{X} + \\alpha \\mathbf{1}_R)^{-1} \\mathbf{X}^Ty\n",
    "\\end{equation}\n",
    "\n",
    "Möchten wir alle Modellkoeffizienten, inklusive $\\beta_0$, auf einmal optimieren, ist $\\mathbf{1}_R$ dabei im Grunde die $(p+1 \\times p+1)$-dimensionale Einheitsmatrix. Allerdings muss das Element $(0,0)$ gleich $0$ gesetzt, dies um den Koeffizienten $\\beta_0$ nicht zu regularisieren. $\\mathbf{1}_R$ ist also:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{1}_r = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & \\cdots & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & 0 & \\cdots & 0 & \\cdots & 0 \\\\\n",
    "0 & 0 & 1 & \\cdots & 0 & \\cdots & 0 \\\\\n",
    "& &  & \\ddots &  & \\vdots &  \\\\\n",
    "& \\vdots &  &  & \\ddots & 0 & 0 \\\\\n",
    "0 & 0 & \\cdots &  & 0 & 1 & 0 \\\\\n",
    "0 & 0 & \\cdots &  &  & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ce1cea0097dd306bae37aa707509d7a",
     "grade": false,
     "grade_id": "cell-96f631c7b64e1008",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Bei standardisierten Inputdaten** kann alternativ auch der oben erwähnte Weg beschritten werden, bei welchem man $\\beta_0 = \\frac{1}{N} \\sum_{i=1}^{N}y^{(i)}$ berechnet, die Spalte von $1$-en $\\mathbf{X}$ nicht vorangestellt und schliesslich eine Normalengleichung verwendet wird mit unveränderter $(p \\times p)$-dimensionaler Einheitsmatrix $\\mathbf{1}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (\\mathbf{X}^T \\mathbf{X} + \\alpha \\mathbf{1})^{-1} \\mathbf{X}^Ty\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "559607c8ad5b4c9a8c035a1a247c5389",
     "grade": false,
     "grade_id": "cell-d5542f16df132ac2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Du bist frei in diesem Aufgabenblatt einen beliebigen (korrekten) Weg für Gradient Descent und Normalengleichung zu implementieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1990a439821fa22a627e9edab2889c6",
     "grade": false,
     "grade_id": "cell-90a0349ba31ff547",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b6aebd04e1ba9813099bb6eb5f5acac",
     "grade": false,
     "grade_id": "cell-5c1385bbe5275343",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Aufgabe**\n",
    "\n",
    "Leite für die obige (exakt) Ridge Regression-Kostenfunktion den Gradienten und die Normalengleichung analytisch her. Leite den vollständigen Gradienten, inklusive $\\beta_0$ her.\n",
    "\n",
    "(Schreibe die Herleitung in LaTex-Notation in die folgende Zelle).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0769244e3e3c99a8694faea5fd7666a3",
     "grade": true,
     "grade_id": "cell-7ccdee03426da874",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7862128da12627f7e1267c6fffa832d5",
     "grade": false,
     "grade_id": "cell-c7df714181768d2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 2 (7 Punkte)\n",
    "\n",
    "Komplettiere die folgende Klasse so, dass sie bei Wahl der entsprechenden Initialisierungsoption, das Ausführen der `fit`-Methode die Kostenfunktion der Ridge Regression-Kostenfunktion mit Gradient Descent (`gd`) oder der regularisierten Normalengleichung (`neq`) optimiert, für exakt die Kostenfunktion aus Aufgabe 1.\n",
    "\n",
    "Verwende den Entwicklungsdatensatz (`Xdev, ydev`) und zeige damit, dass\n",
    "- deine Implementierung für Gradient Descent erfolgreich konvergiert mit Hilfe einer Visualisierung.\n",
    "- du mit Normalengleichung und Gradient Descent praktisch die gleiche (korrekte) Lösung findest (verwende dazu `np.testing.assert_array_almost_equal`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c959e81f62c3d5cfc344e1f91d722047",
     "grade": true,
     "grade_id": "cell-a2d400c29bedb11c",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Development data set\n",
    "Xdev, ydev = make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=4,\n",
    "    random_state=4,\n",
    "    n_informative=3,\n",
    "    effective_rank=2, \n",
    ")\n",
    "\n",
    "class RidgeRegression(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, opt_method='gd', alpha=1., eta=0.01, maxsteps=100, eps=0.00000001):\n",
    "        '''Implements a Ridge Regression estimator.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        alpha:      Regularization proportionality factor. Larger values\n",
    "                    correspond with stronger regularization.\n",
    "        opt_method: Optimization method to choose for the cost function.\n",
    "                    Can be either 'gd' (Gradient Descent) or 'neq' (Normal equation).\n",
    "        maxsteps:   Maximum number of Gradient Descent steps to take.\n",
    "        eps:        Epsilon, length of gradient to be reached with Gradient\n",
    "                    Descent.\n",
    "        eta:        Fixed step length to take at each gradient descent\n",
    "                    iteration.\n",
    "        '''\n",
    "        # parameters\n",
    "        self.alpha = alpha\n",
    "        self.opt_method = opt_method\n",
    "        self.maxsteps = maxsteps\n",
    "        self.eps = eps\n",
    "        self.eta = eta\n",
    "        # attributes\n",
    "        # model coefficients\n",
    "        self.beta_ = None\n",
    "        # values of cost function along gradient descent iterations\n",
    "        self.costs_ = []\n",
    "        \n",
    "    @staticmethod \n",
    "    def ridge_cost_function(beta,X,y,alpha):\n",
    "        '''Computes and returns the value of the ridge regression cost function.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "            \n",
    "    @staticmethod\n",
    "    def ridge_gradient(beta,X,y,alpha):\n",
    "        '''Computes and returns the gradient of the ridge regression cost function.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "                \n",
    "    def ridge_normalequation(self,X,y):\n",
    "        '''Computes the coefficients of the ridge regression cost function\n",
    "        using the normalequation.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        '''Fits the cofficients of the ridge regression model to the data.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return self\n",
    "        \n",
    "    def gradient_descent(self,X,y):\n",
    "        '''Computes the coefficients of the ridge regression cost function\n",
    "        using gradient descent.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def predict(self,X):\n",
    "        '''Computes the predictions of the current model.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def score(self,X,y):\n",
    "        '''Returns R^2 for given input/output data given the model\n",
    "        coefficients.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @staticmethod\n",
    "    def approximate_gradient(beta,X,y,alpha,epsilon=0.00001):\n",
    "        '''Approximates the gradient with finite differences.\n",
    "        \n",
    "        You can use this method to check your gradident.\n",
    "        '''\n",
    "        X_ = np.hstack((np.ones((X.shape[0],1)),X))\n",
    "        grad_approx = []\n",
    "        cf = RidgeRegression.ridge_cost_function\n",
    "        for i,b in enumerate(beta):\n",
    "            eps = np.zeros(beta.shape[0])\n",
    "            eps[i] += epsilon\n",
    "            print(eps)\n",
    "            grad_approx.append(\n",
    "                (cf(beta+eps,X_,y,alpha)-cf(beta-eps,X_,y,alpha))/(2*epsilon)\n",
    "            )\n",
    "        return np.array(grad_approx)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b607ff8f28974c251054b8a6a330e55c",
     "grade": true,
     "grade_id": "cell-81c823ea82f9d89d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0087251bc3b4db086d4d70dfdcf235bc",
     "grade": true,
     "grade_id": "cell-3d3274876d0ab310",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "999b50219c8a5a245db6b959b0eed8d0",
     "grade": false,
     "grade_id": "cell-b4d4775c0c68fd22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 3 (4 Punkte)\n",
    "\n",
    "Alternative Formulierungen der $l_2$-regularisierten linearen Regression werden manchmal wie folgt gewählt:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\beta) = \\frac{2}{N} \\sum_{i=1}^{N} (y^{(i)}-\\beta_0 - \\mathbf{\\beta^Tx^{(i)}})^2 + 2\\alpha\\sum_{j=1}^{p} \\beta_j^2 \n",
    "\\end{equation}\n",
    "\n",
    "mit $\\mathbf{\\beta} = (\\beta_1, \\dots, \\beta_p)$.\n",
    "\n",
    "Erkläre was sich hier im Vergleich zur Formulierung aus Aufgabe 1 verändert hat und was die Implikationen davon sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5276ea48743e703338abb0b7cfbbb65d",
     "grade": true,
     "grade_id": "cell-830d01606b6c474c",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c19700d654bb834438cf53b0f468c263",
     "grade": false,
     "grade_id": "cell-6c9e67b3a9abfa89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 4 (6 Punkte)\n",
    "\n",
    "Mit Hilfe des folgenden Codes kannst du einen einfachen kleinen Datensatz (`[Xdev, ydev]`) erzeugen. Erstelle ein polynomiales Regressionsmodell vom Grade 15 für diese Daten und zeichne die Vorhersagen auf dem Intervall der vorhandenen Datenpunkte als Linie zusammen mit den Daten als Punkte in einen Plot.\n",
    "\n",
    "Verwende nun Ridge Regression, um das polynomiale Regressionsmodell vom Grade 15 zu regularisieren. Wähle 3 sinnvolle Regularisierungsstärken, die es dir ermöglichen den Effekt der Regularisierung zu illustrieren und erkläre und diskutiere diesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f7c46d249acb9a24d2e116fa2da7261",
     "grade": false,
     "grade_id": "cell-8a017c3d167c645d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "NPOINTS = 20\n",
    "RND_SEED = 4\n",
    "np.random.seed(RND_SEED)\n",
    "Xdev = np.random.rand(NPOINTS)*10\n",
    "np.random.seed(RND_SEED)\n",
    "ydev = 0.1*Xdev**3+4*Xdev**2-Xdev+2+np.random.randn(NPOINTS)*50\n",
    "Xdev = Xdev.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2b6823466314e1be3c4ff4ff573c227",
     "grade": true,
     "grade_id": "cell-9f178af0e88fb0d7",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61133dad2b1f290e8b7791207e11a99d",
     "grade": true,
     "grade_id": "cell-5608bf7c5e895cd3",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a45c73f9f0794d8afc8a3c1c2aa55b17",
     "grade": false,
     "grade_id": "cell-7473cc68b3842cbc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 5 (3 Punkte)\n",
    "\n",
    "Betrachten wir anstelle der Ridge Regression Kostenfunktion Lasso, so können wir nicht genauso Gradient Descent verwenden, um die Modellkoeffizienten zu optimieren. Wie schaut die Lasso Kostenfunktion aus und warum ist dies nicht möglich? Wie verändern sich die Eigenschaften der Lösung, i.e. der Koeffizienten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2bb6bdaf8a78a26eb53e65fb0e45a4a4",
     "grade": true,
     "grade_id": "cell-5bfc2127b37e59fe",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "957645a69ac061344bbe301afe630274",
     "grade": false,
     "grade_id": "cell-9f4b6931c8da3b4a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 6 (4 Punkte)\n",
    "\n",
    "Berechne und zeichne den Koeffizientenpfad (siehe beispielsweise Hastie et al., 'Elements of Statistical Learning', 2008, pp 65, e.g. Figure 3.8) deiner Ridge Regression-Implementierung und jenen von Lasso auf dem Entwicklungsdatenset. Zeichne auf der x-Achse den negativen Logarithmus der Regularisierungsstärke $\\alpha$. Als Lasso-Implementierung kannst du die entsprechende scikit-learn-Klasse verwenden.  \n",
    "\n",
    "Wie interpretierst du diese Grafik? Unter welchen Umständen ist Lasso also interessant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a2c67e231912e8ae2c5914ae653220c",
     "grade": true,
     "grade_id": "cell-137cb1b500230850",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8955188e07ed3c65093505a18b5ed32d",
     "grade": true,
     "grade_id": "cell-0c2ee2aa3dc5618c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c13cd58ea3110602aae6c8842461354",
     "grade": false,
     "grade_id": "cell-1b9e59af6e732fbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 7 (4 Punkte)\n",
    "\n",
    "Lade den Datensatz `data/merc.csv` und verschaffe dir einen Überblick durch explorative Datenanalyse. Unser Ziel wird es sein, den Preis (`Offers Price`) der PKW vorherzusagen unter Verwendung der übrigen Attribute. Teile deine Überlegungen zu diesem Problem.  \n",
    "\n",
    "Unterteile den Datensatz nun noch in Trainings- und Testdaten (80:20) für die weiteren Aufgaben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "117ff7d7c33336ff110c0f64f5cb3277",
     "grade": true,
     "grade_id": "cell-6c5aa2e6760d060a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5891794bd0414ce50bc4976145f6ae1c",
     "grade": true,
     "grade_id": "cell-f272aa70fd3d4324",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1307eaf470487ee49a2ffcbc281568e",
     "grade": false,
     "grade_id": "cell-0d74f2c6716125fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 8 (5 Punkte)\n",
    "\n",
    "Erstelle ein erstes einfaches lineares Modell für `Offers Price`, bloss mit der einen Input-Variablen `Mileage`.  \n",
    "\n",
    "Untersuche mittels diagnostischer Plots für die unregularisierte OLS-Lösung die Modell-Annahmen eines linearen Modells. Schau dir dazu *Kapitel 4 - Residuenanalyse* im Skript von Werner Stahel an, wenn du Anleitung möchtest.    \n",
    "\n",
    "Nimm, falls sinnvoll, Variablen-Transformationen vor, um dein Modell zu verbessern und untersuche den Effekt. Erkläre dein Vorgehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d715ac0655f9819f758e5ae5cf6befe2",
     "grade": true,
     "grade_id": "cell-4ab85dc90613cfa9",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0ca5bb89e8f035c6adfcd22c41a72e0",
     "grade": true,
     "grade_id": "cell-bbc2fe849cb0c778",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc6f1defb6a043f09582d1747ed8d89b",
     "grade": false,
     "grade_id": "cell-bb7f260af63b43e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 9 (5 Punkte)\n",
    "\n",
    "Entwickle nun dein bestes Ridge-Regression-Modell im Sinne von MAE auf dem Trainingsdatensatz. Du darfst durch Feature-Transformation beliebige weitere Attribute hinzufügen. Gebe $R^2$ und MAE auf dem Testdatensatz an.\n",
    "\n",
    "Erstelle einen Scatterplot in welchem du den geschätzen Output ($\\hat y$) auf der y- und den wahren Output ($y$) auf der x-Achse zeichnest. Füge dem Titel die Werte von MAE und $R^2$ zu. Interpretieren den Plot.\n",
    "\n",
    "Zur Optimierung der Hyperparameter kannst du scikit-learn-Funktionalität verwenden.  \n",
    "\n",
    "Erkläre dein Vorgehen und diskutiere das erzielte Resultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b86daa3fcfc63a0eb7bb492efa224e0f",
     "grade": true,
     "grade_id": "cell-f504f0d7bf181ad9",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "163fafbb3f9dcf3ff6c4c5b6e5409b24",
     "grade": true,
     "grade_id": "cell-5da408bfe5ef6f3e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "899f346fff56dc7c717e6e15339713de",
     "grade": false,
     "grade_id": "cell-c6b258d9b1937ed4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 10 (7 Punkte)\n",
    "\n",
    "Nun betrachten wir erneut dein bestes Ridge-Regression-Modell etwas vertieft. Trainiere diesen Modell-Ansatz auf jeweils $[\\frac{1}{10}, \\frac{2}{10}, \\frac{3}{10}, .., \\frac{10}{10}]$ der Trainingsdaten. Erstelle nun einen Plot bei welchem der Wert der Kostenfunktion für die Trainings- und Testdaten auf der y-Achse und der Trainingsdatenanteil auf der x-Achse liegen möge. Zeichne also zwei Kurven in dieses Koordinatensystem.\n",
    "\n",
    "Schau dir dazu das Video von [Kilian Weinberger zu Model Selection](https://www.youtube.com/watch?v=a7cofmFgwIk&list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS&index=22) an.  \n",
    "\n",
    "Interpretiere und diskutiere nun deine Einsichten zu Model Selection, Bias & Variance und Grösse des Datensatzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf3649cb0c5709a1c81595fce3d0f092",
     "grade": true,
     "grade_id": "cell-6767f2ee0ce3e193",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "912c0723629359c5e64a8f02b63364ed",
     "grade": true,
     "grade_id": "cell-90ae18e2a93989e5",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5df4e7be3a020149f416ec43689f70f8",
     "grade": false,
     "grade_id": "cell-a79df7de2f3df41d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Aufgabe 11 (6 Punkte)\n",
    "\n",
    "Was ist das beste Modell für die Output-Variable `Offers Price` im Sinne von MAE, das du ohne Einschränkungen finden kannst?\n",
    "\n",
    "Vergleiche dazu mindestens drei weitere Ansätze miteinander. Zeichne für jeden der Ansätze den $\\hat y$ vs $y$ Plot.  \n",
    "\n",
    "Hierzu kannst du scikit-learn Funktionalität verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a0d4fae5a64ab17bb6048b0ae2f924d",
     "grade": true,
     "grade_id": "cell-1d1aaa8430ae2ca1",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
